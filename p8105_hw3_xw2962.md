p8105_hw3_xw2962
================
Xiaoyu Wu
2023-10-06

## Load `Instacart` Dataset

``` r
library(tidyverse)
library(dplyr)
library(p8105.datasets)
data("instacart")
```

## Problem One

#### Short descriptions of the dataset:

Dataset “instacart” has 15 variables: order_id, product_id,
add_to_cart_order, reordered, user_id, eval_set, order_number,
order_dow, order_hour_of_day, days_since_prior_order, product_name,
aisle_id, department_id, aisle, department. In this dataset, there are
1384617 rows. And there are 15 columns. Here we consider variables
“order_dow”, “order_hour_of_day”,“product id”, “product_name”,
“aisle_id” and “aisle”to be important for analysis.

**Q1: How many aisles are there, and which aisles are the most items
ordered from?**

``` r
n_distinct(instacart$aisle_id)
```

    ## [1] 134

``` r
# find how mant disctint aisles are there 
instacart |>
  group_by(aisle_id) |>
  summarize(n_obs = n()) |>
  arrange(desc(n_obs))
```

    ## # A tibble: 134 × 2
    ##    aisle_id  n_obs
    ##       <int>  <int>
    ##  1       83 150609
    ##  2       24 150473
    ##  3      123  78493
    ##  4      120  55240
    ##  5       21  41699
    ##  6      115  36617
    ##  7       84  32644
    ##  8      107  31269
    ##  9       91  26240
    ## 10      112  23635
    ## # ℹ 124 more rows

``` r
# find which aisle ids are the most items ordered from
instacart |>
  group_by(aisle) |>
  summarize(n_obs = n()) |>
  arrange(desc(n_obs))
```

    ## # A tibble: 134 × 2
    ##    aisle                          n_obs
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ℹ 124 more rows

``` r
# find which aisle names are the most items ordered from
```

#### Description

There are 134 aisles. Aisles with id: 83, 24 and 123 are the most items
ordered from. And their corresponding aisle names are: fresh vegetables,
fresh fruits and packaged vegetables fruits.

**Q2: Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.**

``` r
instacart |> 
  group_by(aisle) |>
  summarize(n_obs = n()) |>
  filter(n_obs>10000) |>
  mutate(aisle = forcats::fct_reorder(aisle, n_obs)) |>
  ggplot(aes(x=aisle,y=n_obs)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Number of Items Ordered in each Aisle",
    x = "Aisle Name",
    y = "Number of Items",
    caption = "Data from the p8105.datasets"
  ) +
   coord_flip()
```

![](p8105_hw3_xw2962_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

#### Comment

For showing the number of items ordered in each aisle, we first grouped
the observations by aisle and summarized the number of observation in
each group. We then filtered out the aisles with more than 10000 items
ordered. Next, we plot a horizontal bar chart with aisle names on the
x-axis and number of observations on the y-axis by ordering aisle names
with most number of observations to aisle names with least number of
observations.

**Q3: Make a table showing the three most popular items in each of the
aisles “baking ingredients”, “dog food care”, and “packaged vegetables
fruits”. Include the number of times each item is ordered in your
table.**

``` r
rank_df_one=instacart |>
      filter(aisle=="baking ingredients"|aisle=="dog food care"|aisle=="packaged vegetables fruits") |>
      group_by(aisle,product_name) |>
      summarize(
        n_obs = n()) |>
      mutate(item_ranking = min_rank(desc(n_obs))) |>
      filter(min_rank(desc(n_obs)) < 4) |>
      arrange(aisle,item_ranking) |>
      knitr::kable(digits = 1)
rank_df_one
```

| aisle                      | product_name                                  | n_obs | item_ranking |
|:---------------------------|:----------------------------------------------|------:|-------------:|
| baking ingredients         | Light Brown Sugar                             |   499 |            1 |
| baking ingredients         | Pure Baking Soda                              |   387 |            2 |
| baking ingredients         | Cane Sugar                                    |   336 |            3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30 |            1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |    28 |            2 |
| dog food care              | Small Dog Biscuits                            |    26 |            3 |
| packaged vegetables fruits | Organic Baby Spinach                          |  9784 |            1 |
| packaged vegetables fruits | Organic Raspberries                           |  5546 |            2 |
| packaged vegetables fruits | Organic Blueberries                           |  4966 |            3 |

#### Comment

For making a table showing the three most popular items in each of the
aisles “baking ingredients”, “dog food care”, and “packaged vegetables
fruits”, we first filtered out the rows with aisle names that are
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
We then grouped the data by aisle and product name, summarizing the
observations in each group and ranking the number of observations in
descending order. We kept the top three products with top three highest
number of observations. Finally, we arranged the table according to
aisle and item ranking. In the table, we can see that: for aisle baking
ingredients, the top three most popular items are Light Brown Sugar,
Pure Baking Soda and Cane Sugar; for dog food care, the top three most
popular items are Snack Sticks Chicken & Rice Recipe Dog Treats,Organix
Chicken & Brown Rice Recipe and Small Dog Biscuits; for packaged
vegetables fruits, the top three most popular items are Organic Baby
Spinach,Organic Raspberries and Organic Blueberries.

**Q4: Make a table showing the mean hour of the day at which Pink Lady
Apples and Coffee Ice Cream are ordered on each day of the week; format
this table for human readers (i.e. produce a 2 x 7 table).**

``` r
day_ave_df=instacart |>
      filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream") |>
      group_by(product_name,order_dow) |>
      summarize(
         mean_hour = mean(order_hour_of_day, na.rm = TRUE)) |>
      select(product_name,mean_hour,order_dow)  |>
      arrange(order_dow) |>
      pivot_wider(
        names_from = product_name,
        values_from = mean_hour) |>
      mutate(
    order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday","3" = "Wednesday","4" = "Thursday","5" = "Friday","6" = "Saturday")) |>
      knitr::kable(digits = 1)
day_ave_df
```

| order_dow | Coffee Ice Cream | Pink Lady Apples |
|:----------|-----------------:|-----------------:|
| Sunday    |             13.8 |             13.4 |
| Monday    |             14.3 |             11.4 |
| Tuesday   |             15.4 |             11.7 |
| Wednesday |             15.3 |             14.2 |
| Thursday  |             15.2 |             11.6 |
| Friday    |             12.3 |             12.8 |
| Saturday  |             13.8 |             11.9 |

#### Comment

For making a table showing the mean hour of the day at which Pink Lady
Apples and Coffee Ice Cream are ordered on each day of the week, we
first filtered out the rows with product names “Pink Lady Apples” and
“Coffee Ice Cream”. Then we grouped our observations by product_name and
order_dow. Next, we summarized the mean of order_hour_of_day in each
group. We also selected product_name,mean_hour,order_dow to be shown in
the table and arranged them by order_dow. For the last step, we used
pivot wider, names mutation and knitr to make our table more readable.
The resulting table have three columns:order_dow, Coffee Ice Cream and
Pink Lady Apples.

## Problem Two

## Load `BRFSS` Dataset

``` r
library(p8105.datasets)
data("brfss_smart2010")
```

``` r
brfss_smart_df=
   brfss_smart2010 |> 
  janitor::clean_names() |> 
# format the data to use appropriate variable names
  filter(topic=="Overall Health") |>
# focus on the “Overall Health” topic
  filter(response=="Excellent"|response=="Fair"|response=="Good"|response=="Poor"|response=="Very good") |>
# include only responses from “Excellent” to “Poor”
  mutate(response=as.factor(response)) |> 
  mutate(response=forcats::fct_relevel(response,c("Poor","Fair","Good","Very good","Excellent")))
# organize responses as a factor taking levels ordered from “Poor” to “Excellent”
brfss_smart_df
```

    ## # A tibble: 10,625 × 23
    ##     year locationabbr locationdesc     class topic question response sample_size
    ##    <int> <chr>        <chr>            <chr> <chr> <chr>    <fct>          <int>
    ##  1  2010 AL           AL - Jefferson … Heal… Over… How is … Excelle…          94
    ##  2  2010 AL           AL - Jefferson … Heal… Over… How is … Very go…         148
    ##  3  2010 AL           AL - Jefferson … Heal… Over… How is … Good             208
    ##  4  2010 AL           AL - Jefferson … Heal… Over… How is … Fair             107
    ##  5  2010 AL           AL - Jefferson … Heal… Over… How is … Poor              45
    ##  6  2010 AL           AL - Mobile Cou… Heal… Over… How is … Excelle…          91
    ##  7  2010 AL           AL - Mobile Cou… Heal… Over… How is … Very go…         177
    ##  8  2010 AL           AL - Mobile Cou… Heal… Over… How is … Good             224
    ##  9  2010 AL           AL - Mobile Cou… Heal… Over… How is … Fair             120
    ## 10  2010 AL           AL - Mobile Cou… Heal… Over… How is … Poor              66
    ## # ℹ 10,615 more rows
    ## # ℹ 15 more variables: data_value <dbl>, confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>, data_value_unit <chr>,
    ## #   data_value_type <chr>, data_value_footnote_symbol <chr>,
    ## #   data_value_footnote <chr>, data_source <chr>, class_id <chr>,
    ## #   topic_id <chr>, location_id <chr>, question_id <chr>, respid <chr>,
    ## #   geo_location <chr>

**Q1: In 2002, which states were observed at 7 or more locations? What
about in 2010?**

``` r
brfss_2002=brfss_smart_df |>
           filter(year==2002) |>
           group_by(locationabbr) |>
           summarize(
             n_obs = n(),
             n_location = n_distinct(locationdesc)) |>
           filter(n_location > 6)  
brfss_2002
```

    ## # A tibble: 6 × 3
    ##   locationabbr n_obs n_location
    ##   <chr>        <int>      <int>
    ## 1 CT              35          7
    ## 2 FL              35          7
    ## 3 MA              40          8
    ## 4 NC              35          7
    ## 5 NJ              40          8
    ## 6 PA              50         10

``` r
brfss_2010=brfss_smart_df |>
           filter(year==2010) |>
           group_by(locationabbr) |>
           summarize(
             n_obs = n(),
             n_location = n_distinct(locationdesc)) |>
           filter(n_location > 6)  
brfss_2010
```

    ## # A tibble: 14 × 3
    ##    locationabbr n_obs n_location
    ##    <chr>        <int>      <int>
    ##  1 CA              60         12
    ##  2 CO              35          7
    ##  3 FL             205         41
    ##  4 MA              45          9
    ##  5 MD              60         12
    ##  6 NC              60         12
    ##  7 NE              50         10
    ##  8 NJ              95         19
    ##  9 NY              45          9
    ## 10 OH              40          8
    ## 11 PA              35          7
    ## 12 SC              35          7
    ## 13 TX              80         16
    ## 14 WA              50         10

#### Comment

In 2002, six states: CT,FL,MA,NC,NJ and PA were observed at 7 or more
locations.

In 2010, fourteen states: CA,CO,FL,MA,MD,NC,NE,NJ,NY,OH,PA,SC,TX and WA
were observed at 7 or more locations.

**Q2: Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state **

``` r
        brfss_smart_df |>
        filter(response=="Excellent") |>
        group_by(locationabbr,year) |>
        summarize(
        response=response,
        year=year,
        mean_data = mean(data_value, na.rm = TRUE)) |>
        select(response, year,locationabbr,mean_data) |>
        rename(state=locationabbr) |>
        ggplot(aes(x = year, y = mean_data,color=state)) + 
        geom_line() +
        labs(
    title = "Average Value over Time within a State",
    x = "Year",
    y = "Mean Value",
    caption = "Data from the p8105.datasets"
  )+
  viridis::scale_color_viridis(
      name = "State",
      discrete=TRUE)
```

    ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in
    ## dplyr 1.1.0.
    ## ℹ Please use `reframe()` instead.
    ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`
    ##   always returns an ungrouped data frame and adjust accordingly.
    ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
    ## generated.

    ## `summarise()` has grouped output by 'locationabbr', 'year'. You can override
    ## using the `.groups` argument.

![](p8105_hw3_xw2962_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

#### Comment

For cleaning the dataset, we first filtered out the observations with
responses that are only excellent. Then we grouped the observations by
locationabbr and year. Next, we calculated the mean of data_value
according to this grouping. Finally, we selected the variables we want
and renamed “locationabbr” to “state”.

For making the “spaghetti” plot, we set the aesthetics so that x-axis
showing the year variable, y-axis showing the mean_data variable and
color takes the state variable. Then, we chose the geom_line, labeling
the axises and graph, as well as setting the color scale.

**Q3: Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data_value for responses (“Poor” to “Excellent”) among
locations in NY State.**

``` r
distribution_df = brfss_smart_df|>
  filter(
    locationabbr == "NY"
  )|>
  filter(
    year == 2006 | year == 2010)

distribution_plot= distribution_df |>
  ggplot(aes(x = data_value, fill = locationdesc)) + 
  geom_density(alpha = .3, adjust = .75)+
  facet_grid(. ~ year)

distribution_plot
```

![](p8105_hw3_xw2962_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

#### Comment
