---
title: "p8105_hw3_xw2962"
author: "Xiaoyu Wu"
date: "2023-10-06"
output: github_document
---

## Load `Instacart` Dataset

```{r,message=FALSE}
library(tidyverse)
library(dplyr)
library(p8105.datasets)
data("instacart")
```

## Problem One 

#### Short descriptions of the dataset:

Dataset "instacart" has `r ncol(instacart)` variables: `r colnames(instacart)`. In this dataset, there are `r nrow(instacart)` rows. And there are `r ncol(instacart)` columns. Here we consider variables "order_dow", "order_hour_of_day","product id", "product_name", "aisle_id" and "aisle"to be important for analysis.

**Q1: How many aisles are there, and which aisles are the most items ordered from?**
```{r}
n_distinct(instacart$aisle_id)
# find how many distinct aisles are there 
instacart |>
  group_by(aisle_id) |>
  summarize(n_obs = n()) |>
  arrange(desc(n_obs))
# find which aisle ids are the most items ordered from
instacart |>
  group_by(aisle) |>
  summarize(n_obs = n()) |>
  arrange(desc(n_obs)) |>
  head() |>
  knitr::kable()
# find which aisle names are the most items ordered from
```

#### Description

There are `r n_distinct(instacart$aisle_id)` aisles. Aisles with id: 83, 24 and 123 are the most items ordered from. And their corresponding aisle names are: fresh vegetables, fresh fruits and packaged vegetables fruits.     

**Q2: Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.**
```{r}
instacart |> 
  group_by(aisle) |>
  summarize(n_obs = n()) |>
  filter(n_obs>10000) |>
  mutate(aisle = forcats::fct_reorder(aisle, n_obs)) |>
  ggplot(aes(x=aisle,y=n_obs)) + 
  geom_bar(stat = "identity") +
  labs(
    title = "Number of Items Ordered in each Aisle",
    x = "Aisle Name",
    y = "Number of Items",
    caption = "Data from the p8105.datasets"
  ) +
   coord_flip()
```

#### Comment 

For showing the number of items ordered in each aisle, we first grouped the observations by aisle and summarized the number of observation in each group. We then filtered out the aisles with more than 10000 items ordered. Next, we plot a horizontal bar chart with aisle names on the x-axis and number of observations on the y-axis by ordering aisle names with most number of observations to aisle names with least number of observations.  

**Q3: Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.**
```{r, message=FALSE}
rank_df_one=instacart |>
      filter(aisle=="baking ingredients"|aisle=="dog food care"|aisle=="packaged vegetables fruits") |>
      group_by(aisle,product_name) |>
      summarize(
        n_obs = n()) |>
      mutate(item_ranking = min_rank(desc(n_obs))) |>
      filter(min_rank(desc(n_obs)) < 4) |>
      arrange(aisle,item_ranking) |>
      knitr::kable(digits = 1)
rank_df_one
```

#### Comment 

For making a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, we first filtered out the rows with aisle names that are “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. We then grouped the data by aisle and product name, summarizing the observations in each group and ranking the number of observations in descending order. We kept the top three products with top three highest number of observations. Finally, we arranged the table according to aisle and item ranking. 
In the table, we can see that: for aisle baking ingredients, the top three most popular items are Light Brown Sugar, 
Pure Baking Soda and Cane Sugar; for dog food care, the top three most popular items are Snack Sticks Chicken & Rice Recipe Dog Treats,Organix Chicken & Brown Rice Recipe and Small Dog Biscuits; for packaged vegetables fruits, the top three most popular items are Organic Baby Spinach,Organic Raspberries and Organic Blueberries. 

**Q4: Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).** 
```{r, message=FALSE}
day_ave_df=instacart |>
      filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream") |>
      group_by(product_name,order_dow) |>
      summarize(
         mean_hour = mean(order_hour_of_day, na.rm = TRUE)) |>
      select(product_name,mean_hour,order_dow)  |>
      arrange(order_dow) |>
      pivot_wider(
        names_from = product_name,
        values_from = mean_hour) |>
      mutate(
    order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday","3" = "Wednesday","4" = "Thursday","5" = "Friday","6" = "Saturday")) |>
      knitr::kable(digits = 1)
day_ave_df
```

#### Comment

For making a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week, we first filtered out the rows with product names "Pink Lady Apples" and "Coffee Ice Cream". Then we grouped our observations by product_name and order_dow. Next, we summarized the mean of order_hour_of_day in each group. We also selected product_name,mean_hour,order_dow to be shown in the table and arranged them by order_dow. For the last step, we used pivot wider, names mutation and knitr to make our table more readable. The resulting table have three columns:order_dow, Coffee Ice Cream and Pink Lady Apples.      

## Problem Two

## Load ` BRFSS` Dataset

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

```{r}
brfss_smart_df=
   brfss_smart2010 |> 
  janitor::clean_names() |> 
# format the data to use appropriate variable names
  filter(topic=="Overall Health") |>
# focus on the “Overall Health” topic
  filter(response=="Excellent"|response=="Fair"|response=="Good"|response=="Poor"|response=="Very good") |>
# include only responses from “Excellent” to “Poor”
  mutate(response=as.factor(response)) |> 
  mutate(response=forcats::fct_relevel(response,c("Poor","Fair","Good","Very good","Excellent")))
# organize responses as a factor taking levels ordered from “Poor” to “Excellent”
brfss_smart_df
```

**Q1: In 2002, which states were observed at 7 or more locations? What about in 2010?**
```{r}
brfss_2002=brfss_smart_df |>
           filter(year==2002) |>
           group_by(locationabbr) |>
           summarize(
             n_location = n_distinct(locationdesc)) |>
           filter(n_location > 6) |>
           arrange(n_location) |>
           knitr::kable()
brfss_2002

brfss_2010=brfss_smart_df |>
           filter(year==2010) |>
           group_by(locationabbr) |>
           summarize(
             n_location = n_distinct(locationdesc)) |>
           filter(n_location > 6) |>
           arrange(n_location) |>
           knitr::kable()
brfss_2010
```

#### Comment
In 2002, six states: CT,FL,MA,NC,NJ and PA were observed at 7 or more locations.

In 2010, fourteen states: CA,CO,FL,MA,MD,NC,NE,NJ,NY,OH,PA,SC,TX and WA were observed at 7 or more locations.

**Q2: Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state.**
```{r, warning=FALSE,message=FALSE}
        brfss_smart_df |>
        filter(response=="Excellent") |>
        group_by(locationabbr,year) |>
        summarize(
        response=response,
        year=year,
        mean_data = mean(data_value, na.rm = TRUE)) |>
        select(response, year,locationabbr,mean_data) |>
        rename(state=locationabbr) |>
        ggplot(aes(x = year, y = mean_data,color=state)) + 
        geom_line() +
        labs(
    title = "Average Value over Time within a State",
    x = "Year",
    y = "Mean Value",
    caption = "Data from the p8105.datasets"
  )+
  viridis::scale_color_viridis(
      name = "State",
      discrete=TRUE)
```

#### Comment

For cleaning the dataset, we first filtered out the observations with responses that are only excellent. Then we grouped the observations by locationabbr and year. Next, we calculated the mean of data_value according to this grouping. Finally, we selected the variables we want and renamed "locationabbr" to "state".

For making the “spaghetti” plot, we set the aesthetics so that x-axis showing the year variable, y-axis showing the mean_data variable and color takes the state variable. Then, we chose the geom_line, labeling the axises and graph, as well as setting the color scale. 

**Q3: Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.**
```{r}

distribution_df = brfss_smart_df|>
  filter(
    locationabbr == "NY"
  )|>
  filter(
    year == 2006 | year == 2010)

distribution_plot= distribution_df |>
  ggplot(aes(x = response, y = data_value)) + 
  geom_boxplot()+
  facet_grid(. ~ year)+
  labs(
    title = "Distribution of Data value for Responses across NY in 2006 and 2010",
    caption = "Data from the p8105.datasets"
  )
distribution_plot
```

#### Comment

For making the two-panel plot, we first filtered out the rows for which state is NY and then filtered out the rows for which years are 2006 and 2010. Then we set the aesthetics using response as x variable and data_value as y variable, creating boxplots using geom_boxplot. And next we created two seperate panels for year 2006 and year 2010 using facet function. Last, we added some labels to make a better view.

From this plot, we can see that distributions of data values for very good and good groups are the highest in both years. Distributions of data values for excellent groups are among the next highest around 20 to 25 in both years. And the distributions of data values for poor and fair groups are the lowest below 20 in both years.  

## Problem Three

#### Import Participants’ Demographic Data and Clean 
```{r,message=FALSE}
demo_df = 
  read_csv("./data/nhanes_covar.csv", skip = 4)|>
  janitor::clean_names()|>
  drop_na()|>
# exclude those with missing demographic data
  filter(
    age >= 21
  )|>
# exclude participants less than 21 years of age
  mutate(
    sex = recode(sex, "1" = "male", "2" = "female"),
    education = recode(education, "1" = "Less than high school", 
                   "2" = "High school equivalent", 
                   "3" = "More than high school"), 
# encode data with reasonable variable classes, not numeric 
    sex = factor(sex), 
    education = factor(education)
  )
# convert sex and education into factor variables 
demo_df
```
#### Import Participants’ Accelerometer Data and Clean 
```{r,message=FALSE}
accel_df = 
  read_csv("./data/nhanes_accel.csv")|>
  janitor::clean_names() 

accel_df
```
#### Merge `demo_df` and `accel_df`
```{r}
merged_demo_accel_df= left_join(demo_df, accel_df, by = join_by(seqn))

head(merged_demo_accel_df)|>
  knitr::kable()
```
#### Table for the Number of Men and Women in each Education Category
```{r, message=FALSE}
sex_edu_df = merged_demo_accel_df|>
  group_by(sex, education)|>
  summarize(counts = n())|>
  pivot_wider(names_from = sex, values_from = counts) |>
  knitr::kable()
sex_edu_df
```
#### Visualization of the Age Distributions for Men and Women in each Education Category
```{r,message=FALSE}
sex_edu_age_df  = merged_demo_accel_df|>
  group_by(sex, education)|>
  summarize(mean_age = mean(age, na.rm = TRUE))

sex_edu_age_plot = merged_demo_accel_df|>
  ggplot(aes(x = age, color = sex))+
  geom_density()+
  facet_grid(. ~ education)
sex_edu_age_plot
```

#### Comment
We could see in the plot that, for people who have more than high school degree, more of them are younger people. For people who have less than high school degree, more of them are older people.

#### Plot Total Activities (y-axis) against Age (x-axis)
```{r,message=FALSE}
acc_agg_df = merged_demo_accel_df|>
  mutate(
    aggregate_move = rowSums(select(merged_demo_accel_df, starts_with("min")))
  )

aggregated_age_plot = acc_agg_df|>
  ggplot(aes(x = age, y = aggregate_move, color = sex))+
  geom_point(alpha = .5)+
  geom_smooth()+
  facet_grid(. ~ education)
aggregated_age_plot
```

#### Comment
From the plots, we can see that all movements begin decreasing as ages exceed 60. In the high school equivalent and more than high school groups, female has higher movements than male. Whereas in the less than high school group, male's movement exceeds female's movement around age 40.   

#### Three-paneled Plot
```{r,message=FALSE}
group_24_df = merged_demo_accel_df|>
  group_by(education, sex)|>
  summarise(across(starts_with("min"), ~ mean(.), .names = "mean_{.col}"))|>
  pivot_longer(cols = starts_with("mean_"), names_to = "time", values_to = "mean")|>
  mutate(
    time = substring(time, 9), 
    time= as.numeric(time)
  )

group_24_plot = group_24_df|>
  ggplot(aes(x = time, y = mean, color = sex))+
  geom_line()+
  facet_grid(. ~ education)
group_24_plot
```

#### Comment

For people in the group of high school equivalent and more than high school, average movement for female is higher than average movement for male. And people in all these three groups show low movement from 0 to 250min. 
